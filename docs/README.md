# 大数据

> 大数据的关键词：
>
> ​		吞吐量，延迟，并发，传输速度，io，容错性，扩展性，健壮性，持久化，弹性扩展。
>
> 离线，实时
>
> 时效性。
>
> 数据一致性，



## 分布式：





## 离线计算：

> 离线计算一般指通过批处理的方式计算**已经存储至文件，数据库的数据**（不会产生变化）。
>
> 特点：
>
> + 数据不会发生变化
>
> + 计算量级较大，保存时间长
> + 计算时间较长
> + 对时效性不敏感

场景：

计算前一天积累的日志在凌晨进行计算。



## 实时计算：

> 实时计算一般是通过流处理方式计算当日数据（正在产生的数据，源源不断产生）。
>
> 这种数据是会发生变化的。
>
> 准实时计算，采用的框架是Spark等，微批次进行处理。
>
> 特点：
>
> + 局部计算【以单条，微批次，小窗口数据范围进行计算】
> + 消耗资源成本高【24小时不间断运行】
> + 时效性
> + 可视化性
> + 开发成本【不能写sql，需要写代码】
>
> 

实时计算不能基于全部数据进行统计排序分组，只能对小窗口，微批次进行排序。





# 数据库

## OLTP(联机事务处理系统)：

>  与功能、业务强相关的事务查询系统，要保证高并发场景下低时延的查询和处理效率，因此对CPU的性能要求较高 
>
>  存储的是业务数据，记录某类业务事件的发生，such as : 下单，注册，支付等等 
>
>  代表：mysql
>
>  **数据量相对较少，是GB级别的，面向业务开发人员** 

结构化数据，半结构化数据.....

事务

acid

一致性





分布式锁

乐观锁，悲观锁

1）基于数据库实现

2）基于redis缓存实现

3）基于zookeeper实现







时间

mysql中使用timestamp，默认值为当前时间

使用datetime，默认值是null





## 数据库和数据仓库的比较：



**数据库侧重 事务和查询**

**数据仓库侧重于分析。**

> 数据库用于快速事务处理和实时查询，而数据仓库用于综合分析、历史数据分析和复杂查询 

为什么需要数据库：

1. **事务处理**：数据库主要用于支持事务处理（OLTP），它们专注于实时的数据录入、更新和查询。数据库设计和优化针对高并发、低延迟的操作，更适合支持日常的业务活动和交易处理。
2. **实时查询**：数据库通常用于快速的实时查询，提供即时的响应时间，以满足对当前数据的操作和分析需求。





为什么需要数据仓库：

1. 综合性分析： 数据仓库是一个综合的数据存储解决方案，用于集成和整理来自多个数据源的数据 ，提供海量数据的支持
2. 历史数据分析：存储了历史数据，便于用户在时间上分析。
3. 复杂分析需求：数据仓库支持复杂查询、多维分析和高性能数据挖掘。它们提供了强大的聚合、切片和钻取功能，以更好地理解和利用数据。

> 简化就是，数据仓库提供历史数据，和多个数据源的数据并支持复杂功能进行数据分析。



## 关系型数据库和非关系型数据库的区别













# 数据仓库

存储大规模数据，用于支持决策和分析的数据存储解决方案。

## OLAP(联机分析处理系统)：

> 存储**多业务历史**数据，支持复杂的分析操作，侧重决策，并且提供直观易懂的查询结果。
>
> 代表：
>
> hive，clickhouse，elasticSearch
>
> **数据量大，常规是TB级别的，面向分析决策人员** 



## 模型方法论

### ER模型

实体-关系模型，遵循范式，冗余性低，但是查询时会





事实表：

 事实表是数据仓库中存储**度量数据**（measurements）的主要表 ，其数据一般是可变的。

维度表：

 维度表是数据仓库中存储业务过程中的**描述性数据**（descriptive data）的表 ，数据通常是静态的，不经常发生变化。



> 事实表存储了度量数据，用于度量和计量业务过程，而维度表存储了描述性数据，提供了上下文和筛选条件。它们通过关联和共享键来构建数据仓库中的数据模型，支持复杂的数据分析和报告。 



> 事实表：
>
> + 事务事实表【增量同步】
>
> + 周期快照事实表【全量同步】
>
> + 累积快照事实表【全量同步】
>
> 【后面这两个表可能存在也可能不存在，这俩是解决事务事实表缺点的方案】



> 维度表：
>
> 维度退化： 指的是直接把一些简单的维度放在事实表中 

### 维度模型



### 1.1星型模型

 当所有维表都直接连接到 **“事实表”** 上时，整个图解就像星星一样，故将该模型称为星型模型 。

> 星型架构是一种**非正规化**的结构，多维数据集的每一个维度都直接与事实表相连接，**不存在渐变维度**，所以**数据有一定的冗余**。 



### 1.2雪花模型







### 1.3星座模型

又称星系模型







## 为什么要分层

https://baijiahao.baidu.com/s?id=1771016309102798991&wfr=spider&for=pc













# 离线项目

![img](E:\Typora\typora-images\wps1-1694235528512.png)

## 离线架构

![1694440684529](E:\Typora\typora-images\1694440684529.png)











# 实时项目

## spark实时架构

![1694440744026](E:\Typora\typora-images\1694440744026.png)



### 亮点：

1）使用redis存储offset，使消费数据时可以指定消费位置进行消费

2）在数据流过来后，使用阿里的fastjson将流数据转化为JSON对象 JSON.parseObject(value)

3）采用解耦的方式，将中间变量存放在配置文件中，不至于牵一发而动全身

4）实现精确一次消费：

> 同时解决数据丢失和数据重复，即可实现了精确一次消费。
>
> 自动提交offset，默认5s一次提交，但是无论先提交offset还是写出数据都会出现重复消费或漏消费

1. 将提交offset和写出数据做出事务，进行原子绑定

   ​	前提是利用**关系型数据库**的**事务**进行处理

2. 后置提交offset【至少一次消费】 + 幂等

   ​	后置提交需要手动提交偏移量

5）缓冲区问题

1. kafka默认使用异步发送的方式，kafka的生产者会先将消息发送到缓冲区【RecordAccumulator】，当缓冲区写满或到达指定时间，才会将缓冲区数据写到broker。
2. 消息发送到缓冲区中还未写到broker【但此时认为数据已经成功写给kafka】，然后会手动提交offset，如果offset提交成功，但kafka集群故障，缓冲区数据就会丢失。【 如果消息中间件在接收消息之前发生故障，或者在接收消息后未能正确处理消息，那么消息有可能丢失。即使生产者成功发送消息并收到ack，但如果消息中间件在后续处理过程中出现问题，消息也可能丢失 】

解决方法：

1. 改为同步，性能下降【一般不用】
2. 在手动提交offset之前，强制将缓冲区的数据flush到broker中，一个rdd分区后的一批数据，大小可能会超过16kb（缓冲区自动刷），也可能不会超过（轮到  强制flush 生效）

> 此时是消费端消费时手动提交offset，消费的数据又作为生产者发送给kafka【这时候是有缓冲区】



### 日志文件

> 通过flume对数据进行采集，传送给kafka

![1694441445100](E:\Typora\typora-images\1694441445100.png)

日志数据是用户启动，浏览页面，点击等产生的

通过对对象类型的判断进行分流，错误日志是没有公共字段的，他可以直接传递到下游错误日志Topic，其他数据需要封装为对象再用Json.toJsonString发送





在数据转换前，提取本次流中offset的结束点，后面一步才是转换数据结构

```scala
var offsetRanges: Array[OffsetRange] = null // driver
 kafkaDStream = kafkaDStream.transform( // 每批次执行一次
 rdd => {
 println(rdd.getClass.getName)
 offsetRanges = 
rdd.asInstanceOf[HasOffsetRanges].offsetRanges
 rdd
 }
 )
```



### 业务数据

> 业务数据一般是从数据库中进行同步，使用maxwell

![1694487512103](E:\Typora\typora-images\1694487512103.png)



































# MySQL

SQL语言：

1. DDL（Data Definition Language）：DDL 用于定义和管理数据库的结构，例如创建、修改和删除数据库、表、索引、视图等。常见的 DDL 命令包括 **CREATE（创建）、ALTER（修改）和DROP（删除）**。DDL 语句执行后会立即生效，并且会自动提交事务。
2. DML（Data Manipulation Language）：DML 用于操作和处理数据库中的数据，例如插入、更新、删除和查询数据。常见的 DML 命令包括 **INSERT（插入）、UPDATE（更新）、DELETE（删除）和SELECT（查询）**。DML 语句需要通过事务来进行管理，可以使用 COMMIT 提交或 ROLLBACK 回滚来确保数据的一致性和完整性。
3. DCL（Data Control Language）：DCL 用于管理数据库用户访问权限和安全性。DCL 命令包括 **GRANT（授权）、REVOKE（撤销权限）**等，用于控制用户对数据库对象的访问权限，以确保数据库的安全性。
4. TCL（Transaction Control Language）：TCL 用于管理数据库事务的提交和回滚操作。**TCL 命令包括 COMMIT（提交事务）、ROLLBACK（回滚事务）**等，用于确保事务的一致性和完整性。









## sql的执行流程

![1694406669731](E:\Typora\typora-images\1694406669731.png)

> MySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了。 

1）客户端与sql服务器建立连接，账号认证和校验权限

2）客户端发送查询sql给服务器

3）服务器检查 **查询缓存**，如果命中，立刻返回缓存结果，否则进入下一阶段

4）服务器端进行sql解析，预处理，再由优化器生成对应的执行计划。

5）MySQL根据优化器生成的执行计划，再调用存储引擎的API来执行查询。

6）将结果返回给客户端



> 解析器：
>
> 1）此法分析
>
> 2）语法解析【判断sql是否满足mysql语法】
>
> ​	根据sql语句生成一个数据结构，把他叫做解析树



## 主从复制

### 应用场景：

+ 数据库的**热备**
+ 分离：主数据库只负责写入操作，从数据库负责查询工作

### 原理：

1. master主库记录数据变更记录，写进binary log（二进制日志）中

2. slave从库向master发动dump协议，开启一个线程从主库读取bin log文件，拷贝到slave的中继日志（relay log）
3. 将中继日志的时间同步到自己的数据库。



### binlog：

采用分片+索引（快速定位）

二进制日志包括两类文件：二进制日志索引文件（.index后缀），二进制日志文件（.0000*后缀）

mysql中binlog的格式：

statement | mixed | row







# maxwell

简介：

Maxwell 是由美国 Zendesk 公司开源，用 **Java 编写**的 MySQL 变更数据抓取软件。它会**实时监控 Mysql 数据库的数据变更操作**（包括 insert、update、delete），并将变更数据以 JSON格式发送给 Kafka、Kinesi 等**流数据处理平台** 。

## 原理

将自己伪装成slave，并遵循mysql主从复制的协议，从master同步数据











# Hive

## 讲述一下hive的架构原理

![1694410603651](E:\Typora\typora-images\1694410603651.png)

> 用户通过beeline或jdbc等连接hive【本地模式不需要通过HiveServer2】
>
> 1.用户建表create table
>
> 2.metaStore记录对应路径
>
> 3.metaStore映射表关系【记录hdfs的路径映射】
>
> 4.用户编写hql语句发送给服务器
>
> 5.服务器进行一系列解析，优化，执行。
>
> 6.将计算结果返回



## HQL转换为MR流程

+ 解析器（SQLParser）：将sql语句转化为抽象语法树（AST）
+ 语义分析器（Semantic Analyzer）：将AST进一步抽象为QueryBlock（把每一个子查询划分为一个QueryBlock）
+ 逻辑计划生成器（Logical Plan Gen）：由QueryBlock生成逻辑计划
+ 逻辑优化器（Logical Optimizer）：对逻辑计划进行优化
+ 物理计划生成器（Physical Plan Gen）：根据优化后的逻辑计划生成物理计划
+ 物理优化器（Physical Optimizer）：对物理计划进行优化
+ 执行器（Execution）：执行该计划，得到查询结果并返回给客户端



> 逻辑计划（Logical Plan）是指从HiveQL查询语句转换到物理执行计划之前的中间步骤。它表示了查询语句的逻辑结构和操作流程，但并未考虑底层的数据存储和执行细节 



## Hive与数据库比较

只有语言，语法相近，其他方面差距都是比较大的

|          | hive                                                         | 数据库               |
| -------- | ------------------------------------------------------------ | -------------------- |
| 存储位置 | hdfs                                                         | 本地文件系统         |
| 数据更新 | 不建议更新【hdfs不支持修改】                                 | 需要经常修改         |
| 执行延迟 | 延迟高【但是当数据多的时候【超过数据库的处理能力】，hive并行处理速度快，优势明显】 | 一定量级以内，延迟低 |
| 数据规模 | 支持大规模的数据计算                                         | 规模小               |





## Hive可以做哪些优化

### 1）分组聚合时map-side减少shuffle

 Mapper聚合：在每个Mapper节点上，输入数据首先被预处理，并进行本地聚合。这样可以在Mapper节点上尽可能减少或合并相同键的记录，以减少后续的传输量。通常使用哈希表等数据结构来实现本地聚合 

>--启用 map-side 聚合，默认是 true
>set hive.map.aggr=true;



### 2)Map Join

> hive默认Join算法Common Join,通过一个MR完成一个Join操作

优化join最常用的map join，通过两个只有Map阶段的job完成一个join操作。

第一个job读取小表数据，制作为hash  table，并上传到hadoop分布式缓存（本质是hdfs）。

第二个job从分布式缓存中读取小表数据，并缓存到Map Task的内存中，然后扫描大表数据，在map端即可完成关联操作。【前提是 大表join小表的场景】



>--启动 Map Join 自动转换
>set hive.auto.convert.join=true;



### 3)SMB MapJoin

Sort Merge Bucket Map Join(SMB MapJoin)

要求：

+ 参加join的表均为分桶表，且分桶字段为Join的关联字段
+ 两表分桶数呈倍数关系
+ 数据在分桶内是按照关联字段有序的【满足后可以使用数据库中常用的算法之一Sort Merge Join】

>SMB Map Join 无需构建 Hash Table 也无需缓存小表数据，故其对内存要求很低。 
>
>适用于大表 Join 大表的场景



### 4)Reduce并行度

Reduce 端的并行度，也就是 Reduce 个数，可由用户自己指定，也可由 Hive 自行根据该 MR Job 输入的文件大小进行估算。

**如果reduce的个数不合理，那么就会导致 小文件过多**

还有以下影响：

1. 性能下降：Reduce任务的数量增加意味着更多的任务需要执行，这会增加整体作业的执行时间。每个Reduce任务都需要消耗计算资源和内存，而且还需要进行数据传输和合并操作。如果Reduce任务的数量过多，这些开销会变得更加显著，从而导致作业的性能下降。
2. 资源消耗增加：每个Reduce任务都需要占用一定的计算资源和内存。当Reduce任务数量较多时，整个集群的资源消耗会增加，这可能会导致资源不足的问题。如果集群无法为每个Reduce任务提供足够的资源，可能会导致任务的执行失败或延迟增加。
3. 数据倾斜：当Reduce任务的数量过多时，数据分布可能会变得不均匀，即数据倾斜问题。某些Reduce任务可能会收到比其他任务更多的数据，从而导致这些任务的执行时间明显延长，而其他任务则处于空闲状态。这会导致整个作业的执行时间增加，造成性能瓶颈。
4. 额外的网络开销：在Reduce阶段，数据需要从Map任务传递给Reduce任务。当Reduce任务的数量增加时，将会有更多的数据传输发生，这会增加网络开销。数据传输的频率和数据量将大大增加，可能导致网络拥塞和延迟。

> 资源消耗+，性能-，可能会数据倾斜，额外的网络开销

>若使用 Tez 或者是 Spark 引擎，Hive 可根据计算统计信息（Statistics）估算 Reduce 并行度，其估算的结果相对更加准确。

### 5)小文件合并

因为reduce并行度设置不合理【可能是自动估算有问题】，或导致

计算结果出现大量的小文件。

通过计算任务输出的平均大小进行判断，符合条件就会单独启动一个额外的任务进行合并

>--开启合并 map only 任务输出的小文件
>set hive.merge.mapfiles=true;
>--开启合并 map reduce 任务输出的小文件
>set hive.merge.mapredfiles=true;
>--合并后的文件大小
>set hive.merge.size.per.task=256000000;
>--触发小文件合并任务的阈值，若某计算任务输出的文件平均大小低于该值，则触发合并
>set hive.merge.smallfiles.avgsize=16000000;



### 6）谓词下推

尽量将过滤操作前移，以减少后续计算步骤的数据量

开启谓词下推后，无需调整sql语句，hive就会自动将过滤操作尽可能的前移动。

>--是否启动谓词下推（predicate pushdown）优化 
>
>set hive.optimize.ppd = true;

### 7)并行执行

默认Hive同时只会执行一个Stage，但是sql语句可能会包含多个Stage，其中有些Stage并非完全互相依赖，所以有些Stage是可以并行执行的



### 8）CBO优化

> CBO 是指 Cost based Optimizer，即基于计算成本的优化。
>
> 默认自动开启。

计算模型通过对同一SQL语句 计算不同 执行计划的计算成本，并使用

目前 CBO 在 Hive 的 MR 引擎下主要用于 Join 的优化，例如多表 Join 的 Join 顺序。

### 9）列式存储

采用ORC列式存储加快查询速度



### 10）压缩

通过压缩可以减少磁盘IO。



### 11）分桶和分表

1）创建分区表，防止后续全表扫描

2）创建分桶表，对位置的复杂数据进行提前采样

### 12）更换计算引擎

MR：

最多只有一个map和reduce操作，不能进行迭代计算，基于磁盘，落盘的地方比较多，【优点：虽然慢，但是一定能跑出结果】，**一般计算  周，月，年指标**

Spark:

基于内存，提供非常多的算子，map，fliter等，虽然在shuffle中也落盘【但是中间过程是不落盘的】，但是不是所有算子都需要shuffle，DAG有向无环图，兼顾了可靠性和效率，**一般处理天指标**

Tez：

1)**使用DAG描述任务**，好处：减少MR中不必要的中间节点，减少磁盘io和网络io

2）**资源动态调整**，任务运行时也可以根据具体数据量动态调整后续任务的并行度

> Tez并不是完全基于内存的框架。在处理大规模数据时，依然需要使用磁盘来存储和处理无法完全容纳在内存中的数据。特别是对于超过可用内存容量的数据集，Tez会基于磁盘进行溢写操作，以确保数据的完整性和有效的处理。 



### 13)对于几十张表join如何优化

优先考虑：

+ 数据是否可以过滤【过滤掉不需要的数据】
+ 减少join表的数量：在不影响业务前提下，对一些表进行预处理和合并，可以减少join操作。
+ 选择合适的join顺序：将小表放在前面可以减少中间结果的数据量，提高性能

+ 控制reduce任务数量【可以自己设置，也可以MR自动计算，估算出reduce数量，可能会导致小文件增多】



然后考虑  表的数据量 ：

小表join大表：

- 使用Map join：将小表加载到内存中，避免了reduce操作



- 使用Bucketed Map Join：
- 使用sort Merge Join：在map阶段完成排序，减少了reduce阶段的计算量



- 使用分区
- 使用压缩

## hive解决数据倾斜方法

定义：

参与计算的数据分布不均，某个key值或某些key的数量远超其他key，导致在shuffle阶段，大量相同key数据发往同一个reduce，导致其reduce时间远超其他reduce。【其他资源空闲但是不能释放，也不能帮那个未完成的reduce】

> 简化：某个key值数据太多，导致reduce时间太长。
>
> 想：
>
> 为什么不把倾斜的数据单独开一个mr，让其他资源开在另一个mr中，这样就可以提前释放资源了





> 判断任务是产生数据倾斜：
>
> 在yarn上可以看到任务的执行情况，如果其他任务都运行完了，都剩下这一个在运行就是数据倾斜。
>
> 如果剩余很多数据，可能就是程序有问题【排除数据量特别大，需要运行很长时间】



产生数据倾斜的场景：

+ 分组聚合
+ join操作

### 分组聚合导致的数据倾斜

> Hive 中的分组聚合是由一个 MapReduce Job 完成的。
>
> Map 端负责读取数据，并按照分组字段分区，通过 Shuffle，将数据发往 Reduce 端，各组数据在 Reduce 端完成最终的聚合运算。
>
> 若group by分组字段的值分布不均，就可能导致大量相同的key进入同一Reduce，从而导致数据倾斜。

1）判断倾斜的值是否为null

如果倾斜的数据是null，考虑是否可以过滤掉这种数据



2）Map-Side聚合

开启Map-Side聚合后，会在map端完成部分聚合工作，这样就**可能使原来倾斜的数据在初步聚合后不再发生倾斜**。

set hive.map.aggr=true;



3）skew-GroupBy优化

> skew-GroupBy是Hive提供的一个**专门用来解决分组聚合导致的数据倾斜问题的方案**。

原理：

1. 开启两个MR任务
2. 第一个MR：按照随机数分区，将数据分散到Reduce，并完成部分聚合
3. 第二个MR：按照分组字段分区，完成最终聚合。



--启用分组聚合数据倾斜优化
set hive.groupby.skewindata=true;





### join操作导致

1）map join

使用map join 算法，join操作仅在map端就能完成，没有shuffle操作，没有reduce阶段【自然不会产生reduce端的数据倾斜】，**适用于大表join小表发生数据倾斜的场景**。【将小表加载到内存中，在map端直接处理】

2）skew join 

**适用于大表join大表产生数据倾斜的场景**，

原理：

1. 为倾斜的大key单独启动一个**Map Join**任务进行计算，其余key进行正常的Common Join。

> 大表处理：对于大表，Hive会按照正常的流程进行处理，但在遇到倾斜大键时，会触发单独的Map Join任务。这个任务将会将倾斜大键加载到内存中，并与小表进行Join操作 



3)调整sql语句

![1694438757353](E:\Typora\typora-images\1694438757353.png)

![1694438772120](E:\Typora\typora-images\1694438772120.png)

通过  添加随机数 + 扩容实现

> 扩容：
>
> 表b_1添加随机数1，表b_2添加随机数2，再union实现

```sql
hive (default)>
select * from(
	select --打散操作
		concat(id,'_',cast(rand()*2 as int)) id,value
from A
)ta
join(
select --扩容操作
    concat(id,'_',1) id,
value
from B
union all
select
concat(id,'_',2) id,
value
from B
)tb
on ta.id=tb.id;
```





## hive的数据中含有字段的分隔符怎么处理

Hive 默认的字段分隔符为 Ascii 码的控制符\001（^A），建表的时候用 fields terminated by '\001'。

> tips:
>
> ​		如果采用\t 或者\001 等为分隔符，需要要求前端埋点和 JavaEE 后台传递过来的数据必须不能出现该分隔符，通过**代码规范约束**。

​	一旦传输过来的数据含有分隔符，需要在前一级数据中转义或替换（ETL）。通常采用Sqoop和DataX在同步数据时预处理。

























# flume

> flume是一种分布式的数据采集和传输系统。







# Kafka

> 2.8版本以后可以不采取zk了。
>
> 0.9版本之前offset信息保存到zk，0.9之后offset存储在kafka的主题中

定义：

Kafka是一个**分布式**的基于发布/订阅模式的**消息队列**，分布式事件流平台。

应用场景：

+ 缓冲/消峰：当某一时刻，大量数据到来时，kafka可以解决生产，消费对消息处理速度不一致的问题【将消费不掉的数据存储到kafka中】。
+ 解耦：通过kafka作为中间媒介，让两端的组件遵守同样的接口约束即可。
+ 异步通信：允许用户把一个消息放入队列，但并不立即处理它，然后在需要的时候再去处理它们。

消息队列的两种模式：

+ 点对点【消费者主动拉取数据，消息收到后删除消息】，单播
+ 发布/订阅模式【通过订阅主题，主动将数据推送给订阅着，消费者消费数据后，不删除数据】，广播

> 二者的区别在于：
>
> 0）是否为单播（一个消息只能被一个接受者处理）
>
> 1）消费完数据后是否删除数据
>
> 2）一个是推送，一个是拉取





![1694239682191](E:\Typora\typora-images\1694239682191.png)

> ids/[0,1,2]:代表zk集群中的broker节点



>  /brokers/topics/first/partitions/0/state "leader":0,"isr":[0,2] 
>
>  - "leader": 0，表示该分区的当前 Leader 副本所在的 Broker ID 是 0。
>  - "isr": [0,2]，表示该分区的当前处于同步副本集（In-Sync Replicas，简称 ISR）中的 Broker ID 列表是 [0, 2]。ISR 中的副本与 Leader 副本保持同步，可以参与消息的复制和处理。

## kafka的基础架构有哪些

1. producer
2. consumer
3. broker
4. consumer group（CG）：由多个消费者组成，每个消费者负责消费不同分区的数据，一个分区只能被组内的一个消费者消费，但是消费者组之间互不影响
5. topic
6. partition：一个topic可以分为多个分区，默认为1个，每个分区内都是一个有序的队列。
7. leader
8. flower
9. zookeeper
10. replica：每个分区都有若干个副本，默认一个分区有 一个副本【即，一个leader+0个或多个follower副本】。

> 针对leader和flower：
>
> flower是主动从leader中拉取数据







## kafka怎样保证数据精确一次传输

### 生产端

1）使用事务（影响性能）

使用kafka提供的事务【需要先配置并开启事务日志，并且**开启幂等性**】+消费端支持事务

![1694262134315](E:\Typora\typora-images\1694262134315.png)

2）幂等【下游组件】+至少一次【核心：可以重复消费，但是下游只对重复数据处理一次】

开启参数 **enable.idempotence** 默认为 true，false 关闭

开启幂等后，kafka服务端会缓存生产者发来的五个request的元数据【可以保证这五个是有序的】

> 幂等性：指Producer不论向Broker发送多少次重复数据，Broker端都只会持久化一条，保证了不重复

>**重复数据的判断标准**：具有<PID, Partition, SeqNumber>相同主键的消息提交时，Broker只会持久化一条。其 中PID是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number是单调自增的。



> **所以幂等性只能保证的是在单分区单会话内不重复。**
>
> 使用事务可以保证数据在分区间，多会话 内数据不重复



### 消费端

![1694316644056](E:\Typora\typora-images\1694316644056.png)

使用场景：

数据足够少，并且支持事务的数据库

问题与限制
（1）数据必须都要放在一个**关系型数据库**中，无法使用其他功能强大的 nosql 数据库
（2）事务本身性能不好
（3）如果保存的数据量较大一个数据库节点不够，多个节点的话，还要考虑分布式事务的问题。分布式事务会带来管理的复杂性，一般企业不选择使用，有的企业会把分布式事务变成本地事务，例如把 Executor 上的数据通过 rdd.collect 算子提取到 Driver 端，由Driver 端统一写入数据库，这样会将分布式事务变成本地事务的单线程操作，降低了写入的吞吐量。



2.至少一次消费+ 幂等

需要使用的数据库支持幂等性

使用场景：

处理数据较多，或者数据保存在不支持事务的数据库上。 





## 数据有序

分区内有序，分区间无序

如果需要有序，放到同一个分区

> 想要有序的前提，必须是单分区
>
> **想要数据有序，但是存在数据传输失败重试，怎么保证其有序**
>
> 如何保证单分区内数据有序？
>
> 1）设置retries=0，禁止重试【重试会导致乱序。可能会导致数据丢失】
>
> 2）启用幂等
>
> + 设置enable.idempotence=true，启用幂等
> + 设置max.in.flight.requests.per.connection，1.0.X之后，小于等于5
> + 设置retries，保证其大于0
> + 设置acks，保证其为-1

![1694488713908](E:\Typora\typora-images\1694488713908.png)









## kafka负载均衡

生产端，通过分区，指定分区，指定key，自定义分区等

broker，通过自动副本平衡，topic分区







## 数据积压（消费者如何提高吞吐量）

1）kafka 消费能力不足

+ 增加Topic分区数
+ 同时提升消费组的消费者数量。

> 二者需要同时满足，不然并发还是没有提升

2）下游数据处理不及时

+ 提高每批次最大拉取的数据50M【批次拉取过少，导致拉取速度<生产速度，导致积压】
+ 提高拉取数据返回消息的最大条数500条。





## 数据重复问题

> 如果想要kafka保证数据不重复：
>
> 去重=幂等性+事务【但是代价太高】
>
> 解决方式：至少一次+幂等性【这个幂等可能就交给下游完成了】

事务 + 手动提交 offset （enable.auto.commit = false）。 

消费者输出的目的地必须支持事务（MySQL、Kafka）。





1）生产者端

![1694325406553](E:\Typora\typora-images\1694325406553.png)

2）broker端

Leader挂机，只能保证副本数据的一致性，并不能保证不丢失，或不重复



## 采集数据为什么选择kafka

采集层主要可以使用Flume，kafka等技术

Flume：flume是管道流方式，提供很多的默认实现，让用户通过参数部署，以及扩展API。

kafka：kafka是一个可持久化的分布式的消息队列，是一个非常通用的系统。



flume不支持副本事件。



> 二者可以结合使用.
>
> Flume 是一个专注于数据采集和传输的工具，旨在解决实时数据流的收集、过滤和传输问题。Flume 侧重于低延迟、高吞吐量的实时数据传输，以及对日志和事件数据的处理。
>
> 而消息队列（如 Kafka）则是更通用的消息传递系统，旨在**实现可靠的、分布式的消息传输和存储**。可以**支持多个消费者同时订阅相同的消息流**。消息队列通常被用于构建异步通信系统、事件驱动架构、实时流处理等场景。



应用场景：

kafka:

+ 适用于高吞吐量，低延迟的工作负载
+ 解耦消息发送和接收者，因为发送方不需要等待返回值
+ 消息队列
+ 实时流处理



flume：

+ 日志收集和分析
+ 对一些数据进行etl处理的情况
+ 数据采集和传输



>数据被多个系统消费的话，使用 kafka；如果数据被设计给 Hadoop 使用，使用 Flume。



flume的扩展性不行，而kafka提供了强大的扩展api



















## 生产者

### 流程

涉及两个线程：

+ main线程
+ Sender线程

同步发送流程

![1694242115310](E:\Typora\typora-images\1694242115310.png)

**InFlightRequests，默认每个 Broker最多缓存5个请求**【保证这5个请求都是有序的】 ，

> 客户端通过**发送元数据请求**来获取集群的主题等基本信息。
>
> 双端队列RecordAccumulator
>
> main 线程将消息发送给 RecordAccumulator， 
>
> Sender 线程不断从 RecordAccumulator 中拉取消息发送到 Kafka Broker。 

|                    |                                                              |
| ------------------ | ------------------------------------------------------------ |
| buffer.memory      | RecordAccumulator 缓冲区总大小，**默认 32m**。               |
| batch.size         | 缓冲区一批数据最大值，**默认 16k**。                         |
| linger.ms          | 默认值为***0ms***，表示不启用这个                            |
| enable.idempotence | 是否开启幂等。默认开启幂                                     |
| compression.type   | 生产者发送的所有数据的压缩方式。默认是 none，也 就是不支持压缩，支持压缩类型：none，gzip，snappy，lz4和zstd |
| retries            | 当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。默认是 int 最大值，2147483647。 如果设置了重试，还想保证消息的有序性，需要设置MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 |



### 同步发送和异步发送







### 生产者为什么要分区，有哪些分区策略

> + 提高并行度，供消费者组以分区为单位消费
> + 负载均衡



分区策略:

> 1）指定特定分区【场景：数据全局有序时需要将数据发往同一个分区】
>
> 2）没有partition值（未指定分区）但是存在key值：
>
> 将key的hash值与topic的partition数进行取余得到partition值。
>
> 3）既没有分区值也没有key值，采用Sticky Partition（粘性分区器），会**随机选择一个分区**，并尽可能一直使用该分区（直到当前batch或者linger的时间到了），再选择其他分区【如果选到之前用到的，还会重新随机选取】
>
> 4）自定义分区
>
> + （1）定义类实现 Partitioner 接口。实现 implements Partitioner
>
> + （2）重写 partition()方法。 
>
> + （3）使用分区器的方法，在生产者的配置中添加分区器参数。 properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,"com.atgui 
>
> gu.kafka.producer.MyPartitioner"); 



```java
指定分区，此时key是“”空字符
kafkaProducer.send(new ProducerRecord<>("first", 
1,"","atguigu " + i), new Callback() {}
指定key，此时的分区参数没有了
kafkaProducer.send(new ProducerRecord<>("first", 
"a","atguigu " + i), new Callback() {            
```





### 生产者如何提高吞吐量

+ 提高batch.size的值，默认16k，可以提高吞吐量【不能太大，会导致数据传输延迟增加】
+ linger.ms：等待时间，修改为5-100ms
+ 增加缓冲区大小，默认32M，修改为64M
+ 使用压缩，snappy压缩效果好，使数据量减少



### 生产者的ack应答是怎么回事

> ISR队列： 指的是与主副本保持同步的副本集合。
>
> 只有处于ISR队列中的副本才会被认为是“活跃”节点，可以参与数据的复制和读写操作。
>
> 只有当ISR队列中的大多数副本确认接收并写入消息后，主副本才将消息标记为已提交。
>
> 如果副本无法与主副本保持同步，例如响应延迟过高【默认为30s】或发生故障，Kafka会将其移出ISR队列。这样可以确保只有可靠的、与主副本保持同步的副本参与数据的复制和读写操作。



> 副本包括主副本（Leader）和一组追随者副本（Followers）。**主副本负责处理读写请求**，而**追随者副本**负责**复制主副本的数据**。 



ack的应答级别有：

0：**生产者发送过来的数据，不需要等数据落盘应答**【至多一次】

1：**生产者发送过来的数据，不需要等数据落盘应答**

-1（all）：**生产者发送过来的数据，Leader和ISR队列里面的所有节点收齐数据后应答。** 【至少一次】



### 生产者端数据可靠性

**数据完全可靠条件** **=** **ACK级别设置为-1 +分区副本大于等于2 +ISR里应答的最小副本数量大于等于2**



**可靠性总结：** 

可靠性总结：
acks=0，生产者发送过来数据就不管了，可靠性差，效率高；
acks=1，生产者发送过来数据Leader应答，可靠性中等，效率中等；
acks=-1，生产者发送过来数据Leader和ISR队列里面所有Follwer应答，可靠性高，效率低；
在生产环境中，acks=0很少使用；acks=1，一般用于传输普通日志，允许丢个别数据；acks=-1，一般用于传输和钱相关的数据，
对可靠性要求比较高的场景。



**数据重复分析**：

当数据存储成功但是因为leader挂了等问题，导致-1未返回，会重新选举leader，重新发送一遍数据。























## broker

在zookeeper的服务端存储的Kafka相关信息： 

1）/kafka/brokers/ids [0,1,2] 记录有哪些服务器 

2）/kafka/brokers/topics/first/partitions/0/state 

{"leader":1 ,"isr":[1,0,2] } 记录谁是Leader，有哪些服务器可用

3）/kafka/controller  

{“brokerid”:0}  辅助选举Leader

![1694265678670](E:\Typora\typora-images\1694265678670.png)

1）broker启动后去zk注册

2）controller谁先注册是谁的

3）由选举的Controller监听brokers节点变化，并决定Leader的选举

4）Controller选举完上传节点信息到zk

5）如果某个leader挂了，Controller监听到节点变化，获取ISR，选举新的leader【在ISR中存活，再按照AR（该分区的所有副本统称）排在前面的优先】

6）更新leader及ISR。

### 重要参数

|                                         |                                                              |
| --------------------------------------- | ------------------------------------------------------------ |
| replica.lag.time.max.ms                 | 该时间阈值，**默认 30s**。ISR中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。 |
| auto.leader.rebalance.enable            | **默认是 true**。 自动 Leader Partition 平衡。               |
| leader.imbalance.per.broker.percentage  | **默认是 10%**。每个 broker 允许的不平衡的 leader 的比率，超过触发平衡 |
| leader.imbalance.check.interval.seconds | **默认值 300 秒**。检查 leader 负载是否平衡的间隔时间        |
| log.segment.bytes                       | 默认值 1G。segment的大小                                     |
| log.index.interval.bytes                | 默认 4kb。写入4kb的.log数据，就会生成一个索引。              |
| log.cleanup.policy                      | 默认是 delete，表示所有数据启用删除策略； 如果设置值为 compact，表示所有数据启用压缩策略。 |

> kafka中**数据默认保存7天**，分钟和秒默认关闭。
>
> 当数据过大，超过设置的日志大小，默认策略是删除最早的segment，来清理空间。



### 退役旧节点和服役新节点

服役：

1)创建要均衡的主题

2）生成一个负载均衡计划

bin/kafka-reassign-partitions.sh -- bootstrap-server hadoop102:9092 

--topics-to-move-json-file  topics-to-move.json【自定义的】 --broker-list "0,1,2,3" --generate 

3）创建副本存储计划【将上面返回的计划返回】

4）执行并验证

> 退役节点和服役节点差不多，在第2）步broker列表进行删除旧节点



### Topic的优点有哪些

topic可根据业务需求将消息归类，不同的业务可以有不同的策略【消息保留策略】，支持多个消费者同时订阅同一个topic





### 讲述一下kafka的副本

作用是**提高数据可靠性**【太多副本会增加磁盘存储空间，**增加网络上数据传输**，降低效率】

在Kafka 分区中的所有副本统称为 AR（Assigned Repllicas）。 

AR = ISR + OSR

ISR：和leader保持同步的follower集合。

OSR：表示follower与leader副本同步时，延迟过多的副本

### 讲述Leader选举流程

Leader是由最先注册的broker以Controller身份选举出来的。

Controller Leader，负责管理集群 broker 的上下线，所有 **topic 的分区副本分配和 Leader 选举**等工作



### Leader和Follower故障处理细节

![1694270669518](E:\Typora\typora-images\1694270669518.png)

> 为了保证数据一致性，所以可能需要丢失或重复数据

![1694270792252](E:\Typora\typora-images\1694270792252.png)



### Leader Partition负载平衡

> 根据不平衡数推断不平衡率，满足后再进行再平衡

![1694272376143](E:\Typora\typora-images\1694272376143.png)



### kafka的文件采用什么存储机制

防止log文件过大，导致数据定位效率低下

采用：分片+索引

将每个partition分为多个segment，segment包括.index,.log,.timeindex(时间戳索引文件)文件，这些文件存在于一个文件夹segment下，命名规则：topic名称+分区号（myTopic-0）

> segment文件大小默认为1G，每4kb .log数据写一条索引
>
> 细节：
>
> index中，index的名是当前segment第一条数据绝对offset，而index数据中，存储的是相对offset

![1694273093516](E:\Typora\typora-images\1694273093516.png)



### kafka的文件清理策略

Kafka 中默认的日志保存时间为 7 天。

Kafka 中提供的日志清理策略有 delete 和 compact 两种。

1）delete 日志删除：将过期数据删除 【如果完全过期的数据，可以进行删除】

会出现情况：

一个segment中有很多数据，其中一部分数据过期了，一部分数据没有过期

上述数据其实是不能删除的，可以考虑使用压缩。

2）compact 日志压缩











### kafka数据存储在磁盘，为什么还说是高效

1）kafka是分布式集群，可以采用分区技术，并行度高

2）读数据采用稀疏索引，可以快速定位到要消费的数据

3）顺序写磁盘【一直追加数据到文件末端】

4）页缓存+零拷贝

> 页缓存：
>
> 不走应用层的主要原因是性能和效率方面的考虑。通过采用基于TCP协议的二进制协议，Kafka能够实现更高的吞吐量、更低的延迟和更好的可靠性。 
>
> Kafka之所以可以不用走应用层协议，是因为Kafka利用这种基于日志的存储方式和分区副本机制，使得消息可以持久化存储，并且能够提供高吞吐量和低延迟的消息传输。应用程序可以直接使用Kafka提供的API来发送和接收消息，而无需依赖于额外的通信层协议。
>
> 通过绕过应用层协议，Kafka在消息传递过程中减少了额外的开销，提高了传输效率。同时，Kafka还提供了一些高级特性，比如消息的批量处理和压缩，进一步提升了性能和效率。

> 零拷贝：
>
> 避免数据在不同缓冲区之间的拷贝操作来减少数据传输的开销 
>
> 传统的数据传输方式涉及多次数据拷贝，需要先将数据拷贝到应用程序的缓冲区，然后再从应用程序的缓冲区将数据拷贝到网络缓冲区。
>
> 这样的拷贝操作会增加CPU和内存的开销。而**零拷贝技术通过直接将数据从磁盘或网络缓冲区传输到目标缓冲区，避免了中间的拷贝操作**，减少了CPU和内存的使用，提高了数据传输的效率。 

![1694274693473](E:\Typora\typora-images\1694274693473.png)



## 消费者

> offset存储在broker的主题   __consumer_offsets[分区数为50]，
>
> __consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行compact【压缩】，也就是每个 group.id+topic+分区号就保留最新数据。 

> 消费者采取主动拉取数据的方式获取数据【如果使用push方式，broker的发送速率不好确定】

![1694309840791](E:\Typora\typora-images\1694309840791.png)

![1694310242163](E:\Typora\typora-images\1694310242163.png)

>1. 选举leader的算法通常基于以下几个因素进行评估：
>
>  - 活跃性：对于那些已经活跃地消费消息并保持连接的消费者来说，更有可能成为leader。
>  - **优先级**：如果消费者设置了优先级属性，那么具有更高优先级的消费者可能更容易成为leader。
>  - **顺序**：根据消费者加入组的先后顺序，先加入的消费者可能更有机会成为leader。

1)消费者组向coordinator发送JoinGroup请求

2）选出一个consumer作为leader

3）把要消费的topic情况发送给leader消费者

4)leader消费者制定消费方案

5）leader将消费方案发送给coordinator

6）coordinator将消费方案下发给各个consumer

7）每个消费者和coordinator保持心跳（默认3秒），一旦超时（session.timeout.ms=45s），消费者会被移除，并触发**再平衡**【topic副本也存在再平衡】，或者消费者处理消息的时间过长（max.poil.interval.ms 5分钟），也会触发再平衡。

![1694311253962](E:\Typora\typora-images\1694311253962.png)

超过50M，或者500ms，达到500条，会返回数据。



### 重要参数

|                         |                                                              |
| ----------------------- | ------------------------------------------------------------ |
| enable.auto.commit      | **默认值为 true**，消费者会自动周期性地向服务器提交偏移量    |
| auto.commit.interval.ms | 默认5s提交一次偏移量                                         |
| auto.offset.reset       | 当 Kafka 中没有初始偏移量或当前偏移量在服务器中不存在 （如，数据被删除了），该如何处理？ earliest：自动重置偏 移量到最早的偏移量。 latest：默认，自动重置偏移量为最新的偏移量。 none：如果消费组原来的（previous）偏移量不存在，则向消费者抛异常。 anything：向消费者抛异常。 |
| heartbeat.interval.ms   | 消费者与coordinator之间的心跳为3s                            |
| session.timeout.ms      | Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。 |
| max.poll.interval.ms    | 默认5分钟。消费者处理数据的最大时长，超过该值，消费者被移除，执行再平衡 |
| fetch.min.bytes         | 默认1字节，                                                  |
| fetch.max.wait.ms       | 500ms                                                        |
| fetch.max.bytes         | 50M                                                          |
| max.poll.records        | 500条                                                        |



























### 为什么设计消费者组

>使用消费者组可以解决一些问题和提升性能：
>
>+ 提高吞吐量【并行消费，允许多个消费者并行处理来自多个分区的消息】
>
>+ 负载均衡【将主题的分区均匀地分配给消费者组】
>
>
>
>
>**消费者组是逻辑上的一个订阅者**，



### 讲述一下分区的分配以及再平衡

> 挂掉后45s后才能触发再平衡

消费者组的分区分配策略：

1）Range（对分区和消费者排序，通过取余计算每个消费者要消费的个数，按照分区的顺序取对应消费分区的个数）

2）RoundRobin（轮循）

3）Sticky（粘性）

4）CooperativeSticky（合作式粘性）

**默认的策略是Range+CooperativeSticky**，可以多个分区策略混用



> **默认的策略是Range+CooperativeSticky**，也算是默认使用Range策略
>
> **Range容易产生数据倾斜**【如果只有一个topic，消费者可能会多消费一个分区，但是当topic有很多时，消费者就会多消费很多分区】
>
> RoundRobin轮循，是吧所有的partition和所有的consumer都列出来，按照hashcode排序，通过轮循算法分配【不会出现数据倾斜】





### 讲一下offset

offset有两种提交方式：

1）自动提交【基于时间提交的，难以把握offset提交的时机】

enable.auto.commit：是否开启自动提交offset功能，**默认是true** 

auto.commit.interval.ms：自动提交offset的时间间隔，默认是5s



2）手动提交

+ 同步提交【必须等待offset提交完毕后再去消费下一批数据】

+ 异步提交（没有失败重试机制，可能会提交失败）

#### 当 Kafka 中没有初始偏移量（消费者组第一次消费）或服务器上不再存在当前偏移量 时（例如该数据已被删除），该怎么办？

指定offset消费：

auto.offset.reset = earliest | latest | none 默认是 latest。

（1）earliest：自动将偏移量重置为最早的偏移量，--from-beginning。 

（2）latest（默认值）：自动将偏移量重置为最新偏移量。 

（3）none：如果未找到消费者组的先前偏移量，则向消费者抛出异常。 



#### 指定时间消费

需求：在生产环境中，会遇到最近消费的几个小时数据异常，想重新按照时间消费。例如要求按照时间消费前一天的数据，怎么处理？



### 漏消费和重复消费

重复消费【自动提交offset引起】：已经消费了数据，但是 offset 没提交。

漏消费【手动提交offset引起】：先提交 offset 后消费，有可能会造成数据的漏消费。





















# redis

 Lua是一种轻量级、高效的脚本语言,具有可扩展性和可嵌入性 

> 使用eval 通过lua脚本去操作批量数据：
>
> EVAL "local keys = redis.call('keys', 'DIM*'); for i=1,#keys,5000 do redis.call('del', unpack(keys, i, math.min(i+4999, #keys))) end" 0
>
> 







# Zookeeper

协调分布式系统。





# ElasticSearch

> ES是一个实时的分布式搜索和分析引擎。

es被认为是一种OLAP数据库。

原因：

+ 多维度数据分析
+ 实时性
+ 分布式架构
+ 弹性扩展
+ 文本搜索能力



# 小海豚

dolphinscheduler： 任务调度和工作流管理系统 

Azkaban： 任务调度和工作流管理系统 

